{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of `Attention` mechanism in `Transformer` model\n",
    "\n",
    "This implementation is based on only `Torch`, and the reference is [Mistral-7B-v0.1](https://github.com/mistralai/mistral-src/blob/main/mistral/model.py).\n",
    "The attention mechanism in this notebook is **Grouped-Query Attention**.\n",
    "Thus, when `n_kv_heads` is 1, it is equivalent to the **Multi-Query Attention**; when `n_kv_heads` is equal to `n_heads`, it is equivalent to the **Multi-Head Attention**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from simple_parsing.helpers import Serializable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions for Rotary Positional Embedding in the Attention layer\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float) -> torch.Tensor:\n",
    "    freqs = 1.0 / (\n",
    "        theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)\n",
    "    )\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    return torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    q: torch.Tensor,\n",
    "    k: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    q_ = torch.view_as_complex(q.float().reshape(*q.shape[:-1], -1, 2))\n",
    "    k_ = torch.view_as_complex(k.float().reshape(*k.shape[:-1], -1, 2))\n",
    "    freqs_cis = freqs_cis[:, None, :]\n",
    "    q_out = torch.view_as_real(q_ * freqs_cis).flatten(-2)\n",
    "    k_out = torch.view_as_real(k_ * freqs_cis).flatten(-2)\n",
    "    return q_out.to(q.dtype), k_out.to(k.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3, 8]), torch.Size([2, 4, 3, 8]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy q, k, and freqs_cis to test `apply_rotary_emb`\n",
    "_q = torch.randn(2, 4, 3, 8) # batch, seq_len, n_heads, emb_dim\n",
    "_k = torch.randn(2, 4, 3, 8)\n",
    "_freqs_cis = precompute_freqs_cis(8, 4, 10) # emb_dim, seq_len, theta\n",
    "_freqs_cis.shape\n",
    "q_out, k_out = apply_rotary_emb(_q, _k, _freqs_cis)\n",
    "q_out.shape, k_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AttnArgs(Serializable):\n",
    "    block_size: int\n",
    "    emb_dim: int\n",
    "    n_layers: int\n",
    "    head_dim: int\n",
    "    hidden_dim: int\n",
    "    n_heads: int\n",
    "    n_kv_heads: int\n",
    "    norm_eps: float\n",
    "    p_drop: float\n",
    "\n",
    "    rope_theta: Optional[float] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy config to test `AttnArgs`\n",
    "config = AttnArgs(\n",
    "    block_size=4,\n",
    "    emb_dim=8,\n",
    "    n_layers=3,\n",
    "    head_dim=6,\n",
    "    hidden_dim=16,\n",
    "    n_heads=9,\n",
    "    n_kv_heads=3,\n",
    "    norm_eps=1e-6,\n",
    "    p_drop=0.1,\n",
    "    rope_theta=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define an `Attention` layer with grouped-query attention mechanism.\n",
    "The `n_heads` is the number of query heads, and the `n_kv_heads` is the number of key-value heads.\n",
    "Thus, the number of group is `n_kv_heads // n_heads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, config: AttnArgs) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.n_heads: int = config.n_heads\n",
    "        self.n_kv_heads: int = config.n_kv_heads\n",
    "        self.head_dim: int = config.head_dim\n",
    "\n",
    "        self.repeats = self.n_heads // self.n_kv_heads\n",
    "        self.scale = config.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.emb_dim, config.n_heads * config.head_dim, bias=False\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.emb_dim, config.n_kv_heads * config.head_dim, bias=False\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.emb_dim, config.n_kv_heads * config.head_dim, bias=False\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.n_heads * config.head_dim, config.emb_dim, bias=False\n",
    "        )\n",
    "\n",
    "        self.p_drop = config.p_drop\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"attn_bias\",\n",
    "            torch.tril(\n",
    "                torch.ones(config.block_size, config.block_size)\n",
    "            ).view(1, 1, config.block_size, config.block_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freq_cis: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, _ = x.size()\n",
    "\n",
    "        q, k, v = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim)\n",
    "        k = k.view(B, T, self.n_kv_heads, self.head_dim)\n",
    "        v = v.view(B, T, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        q, k = apply_rotary_emb(q, k, freq_cis)\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        k = torch.repeat_interleave(k, repeats=self.repeats, dim=1)\n",
    "        v = torch.repeat_interleave(v, repeats=self.repeats, dim=1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.masked_fill(\n",
    "            self.attn_bias[:, :, :T, :T] == 0, float(\"-inf\")\n",
    "        )\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = F.dropout(attn, self.p_drop, training=self.training)\n",
    "        y = (attn @ v).transpose(1, 2).contiguous().reshape(B, T, -1)\n",
    "        y = F.dropout(y, self.p_drop, training=self.training)\n",
    "        return self.o_proj(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GroupedQueryAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 4, 8)\n",
    "freqs_cis = precompute_freqs_cis(config.head_dim, 4, config.rope_theta)\n",
    "y = model(x, freqs_cis)\n",
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
