[model]

dim = 4
n_layers = 2
head_dim = 4
hidden_dim = 8
n_heads = 4
n_kv_heads = 4
norm_eps = 1e-6
vocab_size = 5

max_batch_size = 0

# For rotary embeddings. If not set, will be infered from sliding window.
rope_theta = 1e-5
# If this is set, use sliding window attention rotating cache.
sliding_window = 0
# If this is set, we will use MoE layers instead of dense layers.
moe = false

[init]
seed = 0